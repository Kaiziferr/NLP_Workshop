{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_workshop_embedding.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP2uHnwsO7mRo1kJF0nmOQX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kaiziferr/NLP_Workshop/blob/master/embedding/01_workshop_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El presente proyecto tiene como finalidad el desarrollo de la incrustración de palabras con keras, a travez de un ejercicio de análisis de sentimiento basico. El tutorial fue tomado de https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
      ],
      "metadata": {
        "id": "uwgy2hjwQJuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.layers import Dense, Flatten, Embedding"
      ],
      "metadata": {
        "id": "JOOpz339Qs1s"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1KBhaUclP5P_"
      },
      "outputs": [],
      "source": [
        "docs = ['¡Bien hecho!', 'Buen trabajo', 'Gran esfuerzo', 'Buen trabajo','¡Excelente!',\n",
        "'Débil','¡Esfuerzo pobre!','no es bueno','trabajo pobre', 'Podría haberlo hecho mejor.']\n",
        "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SoGoRF-6YhbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el vocabulario = 50\n",
        "vocab_size = 50\n",
        "\n",
        "encoded = [text.one_hot(d, vocab_size) for d in docs]\n",
        "print(\"Se definio un vocabulario de 50 para reducir el número de colisiones\")\n",
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9A_RRgQREmE",
        "outputId": "a6920530-2812-4f79-fe7b-646ba9d46c0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se definio un vocabulario de 50 para reducir el número de colisiones\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[31, 8],\n",
              " [18, 38],\n",
              " [5, 26],\n",
              " [18, 38],\n",
              " [28],\n",
              " [28],\n",
              " [31, 34],\n",
              " [48, 24, 35],\n",
              " [38, 34],\n",
              " [37, 25, 8, 45]]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pad Secuencia**\n",
        "---"
      ],
      "metadata": {
        "id": "3445B7wxYnOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "padded_docs = sequence.pad_sequences(encoded, maxlen=max_length, padding='post', truncating='pre')\n",
        "print(\"Las secuencias tienen diferentes longitudes y Keras prefiere que las entradas estén vectorizadas y\\nque todas las entradas tengan la misma longitud.\\nRellenaremos todas las secuencias de entrada para que tengan una longitud de 4. \", \n",
        "      \"\\n\\nEl parametro padding indica el relleno de cada frase, si es posterioro o pre\", padded_docs, '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhQO5owUWNlF",
        "outputId": "48374d9b-240e-4f00-aa40-0325a584f4d2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Las secuencias tienen diferentes longitudes y Keras prefiere que las entradas estén vectorizadas y\n",
            "que todas las entradas tengan la misma longitud.\n",
            "Rellenaremos todas las secuencias de entrada para que tengan una longitud de 4.  \n",
            "\n",
            "El parametro padding indica el relleno de cada frase, si es posterioro o pre [[31  8  0  0]\n",
            " [18 38  0  0]\n",
            " [ 5 26  0  0]\n",
            " [18 38  0  0]\n",
            " [28  0  0  0]\n",
            " [28  0  0  0]\n",
            " [31 34  0  0]\n",
            " [48 24 35  0]\n",
            " [38 34  0  0]\n",
            " [37 25  8 45]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **La capa Embedding y el modelo**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1PQAr_FpYtcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRk-e33EX8Xm",
        "outputId": "2cb7eca6-1f84-400b-8f01-53e6a7cde981"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 4, 8)              400       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('accuracy: %.2f' % (accuracy*100))\n",
        "print('loss: %.2f' % (loss*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSeAbdHdagpM",
        "outputId": "9b33fa87-47c9-4f05-ae1a-909868d887c6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 90.00\n",
            "loss: 21.53\n"
          ]
        }
      ]
    }
  ]
}