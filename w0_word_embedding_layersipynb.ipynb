{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w0_word_embedding_layersipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSQhmhuARP6vBhgPAqXJyT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kaiziferr/NLP_Workshop/blob/master/w0_word_embedding_layersipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC7KxlpzqXoz"
      },
      "source": [
        "La capa de incrustación se inicializa con pesos aleatorios y aprenderá una incrustación para todas las palabras en el conjunto de datos de entrenamiento.\n",
        "\n",
        "- Se puede usar solo para aprender una incrustación de palabras que se puede guardar y usar en otro modelo más adelante.\n",
        "-Se puede utilizar como parte de un modelo de aprendizaje profundo en el que la incorporación se aprende junto con el modelo en sí\n",
        "- Se puede utilizar para cargar un modelo de incrustación de palabras previamente entrenado, un tipo de aprendizaje por transferencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JI8KKwvqrUq"
      },
      "source": [
        "Argumentos\n",
        "- input_dim: tamaño del vocabulario en los datos de texto Por ejemplo, si sus datos están codificados en números enteros con valores entre 0 y 10, entonces el tamaño del vocabulario sería de 11 palabras.\n",
        "- output_dim: este es el tamaño del espacio vectorial en el que se incrustarán las palabras.\n",
        "- input_length : Esta es la longitud de las secuencias de entrada, como definiría para cualquier capa de entrada de un modelo de Keras. Por ejemplo, si todos sus documentos de entrada están compuestos por 1000 palabras, esto sería 1000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ2QpqqRrr6k"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "from keras.preprocessing.text import one_hot, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlNAL3xjtYHY"
      },
      "source": [
        "docs = ['Well done!', 'Good work', 'Great effort', 'nice work', 'Excellent!', 'Weak', 'Poor effort!', 'not good', 'poor work', 'Could have done better.']\n",
        "labels = np.array([1,1,1,1,1,0,0,0,0,0])\n",
        "vocab_size = 50"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMaGB_XJ_WIR",
        "outputId": "9f2eaee8-0b3b-4fcc-c091-602b3777c310"
      },
      "source": [
        "# One-hot codifica un texto en una lista de índices de palabras de tamaño n\n",
        "encoded_docs = [one_hot(d, vocab_size, split=' ') for d in docs]\n",
        "encoded_docs"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 31],\n",
              " [18, 25],\n",
              " [26, 18],\n",
              " [9, 25],\n",
              " [9],\n",
              " [22],\n",
              " [8, 18],\n",
              " [25, 18],\n",
              " [8, 25],\n",
              " [14, 31, 31, 45]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9OMbug7AvKP",
        "outputId": "cfe4313f-c6a8-4285-fdbb-d83952661240"
      },
      "source": [
        "sequence = [[1], [2, 3], [4, 5, 6], [7,8,9,10]]\n",
        "pad_sequences(sequence, padding='post', maxlen=2, dtype='float64', truncating='pre')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.,  0.],\n",
              "       [ 2.,  3.],\n",
              "       [ 5.,  6.],\n",
              "       [ 9., 10.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKcg1q1SvC4Y",
        "outputId": "a7190073-9c0c-4ad8-ae61-ab66a232a1f5"
      },
      "source": [
        "mx_length = 4\n",
        "padded_doc = pad_sequences(encoded_docs, maxlen=mx_length, padding='post', value=0)\n",
        "print(padded_doc, type(padded_doc), padded_doc.shape, padded_doc.ndim)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 8 31  0  0]\n",
            " [18 25  0  0]\n",
            " [26 18  0  0]\n",
            " [ 9 25  0  0]\n",
            " [ 9  0  0  0]\n",
            " [22  0  0  0]\n",
            " [ 8 18  0  0]\n",
            " [25 18  0  0]\n",
            " [ 8 25  0  0]\n",
            " [14 31 31 45]] <class 'numpy.ndarray'> (10, 4) 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCp7wBHSrxsh"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length = mx_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1eb3ZC_stPx",
        "outputId": "27d5479a-748b-4f38-d25a-ea6a231f9b51"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 4, 8)              400       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daNA_D_OwrfV",
        "outputId": "30b07f0a-64c4-471a-b798-7acf9df3eada"
      },
      "source": [
        "model.fit(padded_doc, labels, epochs=50, verbose=0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7eb187b8d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5jVDjOT7lGs",
        "outputId": "771c5e6f-d88d-45e8-9955-25fe6b0f771c"
      },
      "source": [
        "loss, accuracy = model.evaluate(padded_doc, labels,verbose=0)\n",
        "print('Accuracy %f' % (accuracy*100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 69.999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LDsPdECLBmk",
        "outputId": "51863021-c3f4-4170-8d39-500bbaa28845"
      },
      "source": [
        "doc_valida = ['Well work']\n",
        "input_valide_doc = [one_hot(doc_valida[0], vocab_size, split=' ')]\n",
        "padded_doc_valid = pad_sequences(input_valide_doc, padding='post', maxlen = 4, value=0)\n",
        "padded_doc_valid"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8, 25,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XsKLHrdMcO7",
        "outputId": "6cf3c73f-ba8b-4270-e3cd-e24b55b3fb98"
      },
      "source": [
        "model.predict(padded_doc_valid)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5156619]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVWe_R7qUB9E"
      },
      "source": [
        "Keras proporciona una clase Tokenizer que se puede ajustar a los datos de entrenamiento, puede convertir texto en secuencias de manera consistente llamando al método texts_to_sequences () en la clase Tokenizer , y brinda acceso al mapeo del diccionario de palabras a enteros en un atributo word_index ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwoVnAnyT-CM",
        "outputId": "af62fbf4-e563-4e23-f0b0-51683e2c0ee8"
      },
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "t.word_index"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'better': 14,\n",
              " 'could': 12,\n",
              " 'done': 2,\n",
              " 'effort': 4,\n",
              " 'excellent': 9,\n",
              " 'good': 3,\n",
              " 'great': 7,\n",
              " 'have': 13,\n",
              " 'nice': 8,\n",
              " 'not': 11,\n",
              " 'poor': 5,\n",
              " 'weak': 10,\n",
              " 'well': 6,\n",
              " 'work': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2L8e73eUl7D",
        "outputId": "d25f1423-92a9-45f2-d2c5-7c1385d64584"
      },
      "source": [
        "vocab_size = len(t.word_index) + 1\n",
        "vocab_size"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fLmTeCKUq_S",
        "outputId": "99c03e62-54bd-40ab-c97c-9d57623e3b41"
      },
      "source": [
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "encoded_docs"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[6, 2],\n",
              " [3, 1],\n",
              " [7, 4],\n",
              " [8, 1],\n",
              " [9],\n",
              " [10],\n",
              " [5, 4],\n",
              " [11, 3],\n",
              " [5, 1],\n",
              " [12, 13, 2, 14]]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-SYfmdeU4N9",
        "outputId": "35574b35-e01b-458c-fbd7-dc51f74babe7"
      },
      "source": [
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 6  2  0  0]\n",
            " [ 3  1  0  0]\n",
            " [ 7  4  0  0]\n",
            " [ 8  1  0  0]\n",
            " [ 9  0  0  0]\n",
            " [10  0  0  0]\n",
            " [ 5  4  0  0]\n",
            " [11  3  0  0]\n",
            " [ 5  1  0  0]\n",
            " [12 13  2 14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yosHUkzDVWg8",
        "outputId": "21567415-8be0-4e4f-bb2b-1f722a56081a"
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('./data/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.array(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 39103 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APYmSgTdaVQV"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size,100))\n",
        "for word, i in t.word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8ReO4JebjBT",
        "outputId": "40551342-c237-4491-d3c1-4515e30c6b0f"
      },
      "source": [
        "embedding_matrix"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.11619   ,  0.45447001, -0.69216001, ..., -0.54737002,\n",
              "         0.48822001,  0.32246   ],\n",
              "       [-0.2978    ,  0.31147   , -0.14937   , ..., -0.22709   ,\n",
              "        -0.029261  ,  0.4585    ],\n",
              "       ...,\n",
              "       [ 0.05869   ,  0.40272999,  0.38633999, ..., -0.35973999,\n",
              "         0.43718001,  0.10121   ],\n",
              "       [ 0.15711001,  0.65605998,  0.0021149 , ..., -0.60614997,\n",
              "         0.71004999,  0.41468999],\n",
              "       [-0.047543  ,  0.51914001,  0.34283999, ..., -0.26859   ,\n",
              "         0.48664999,  0.55609   ]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUrG922ubfmU",
        "outputId": "0ccc949e-ac7f-4cc2-eeab-50faa416fcb2"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 4, 100)            1500      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 401       \n",
            "=================================================================\n",
            "Total params: 1,901\n",
            "Trainable params: 401\n",
            "Non-trainable params: 1,500\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy8mR5THdjvy",
        "outputId": "63ac6cd7-963c-45da-e99a-3d84434e8451"
      },
      "source": [
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K20klJyHq1Ms"
      },
      "source": [
        "# Referencias\n",
        "- https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
      ]
    }
  ]
}